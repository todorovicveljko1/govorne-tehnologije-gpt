{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacija GPT-2 modela za srpski jezik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cilj projekta jeste treniranje GPT-2 modela na delu srpske wikipedije i generisanje artikala sliƒçnog sastava"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biblioteke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asttokens==2.4.1 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.3 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 2)) (4.12.3)\n",
      "Requirement already satisfied: certifi==2024.2.2 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 3)) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: colorama==0.4.6 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: comm==0.2.2 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: debugpy==1.8.1 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 7)) (1.8.1)\n",
      "Requirement already satisfied: decorator==5.1.1 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 8)) (5.1.1)\n",
      "Requirement already satisfied: executing==2.0.1 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 9)) (2.0.1)\n",
      "Requirement already satisfied: filelock==3.13.3 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 10)) (3.13.3)\n",
      "Requirement already satisfied: fsspec==2024.3.1 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 11)) (2024.3.1)\n",
      "Requirement already satisfied: idna==3.6 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 12)) (3.6)\n",
      "Requirement already satisfied: ipykernel==6.29.4 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 13)) (6.29.4)\n",
      "Requirement already satisfied: ipython==8.22.2 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 14)) (8.22.2)\n",
      "Requirement already satisfied: jedi==0.19.1 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 15)) (0.19.1)\n",
      "Requirement already satisfied: Jinja2==3.1.3 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 16)) (3.1.3)\n",
      "Requirement already satisfied: jupyter_client==8.6.1 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 17)) (8.6.1)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 18)) (5.7.2)\n",
      "Requirement already satisfied: MarkupSafe==2.1.5 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 19)) (2.1.5)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 20)) (0.1.6)\n",
      "Requirement already satisfied: mpmath==1.3.0 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 21)) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 22)) (1.6.0)\n",
      "Requirement already satisfied: networkx==3.2.1 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 23)) (3.2.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 24)) (1.26.4)\n",
      "Requirement already satisfied: packaging==24.0 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 25)) (24.0)\n",
      "Requirement already satisfied: parso==0.8.3 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 26)) (0.8.3)\n",
      "Requirement already satisfied: pillow==10.2.0 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 27)) (10.2.0)\n",
      "Requirement already satisfied: platformdirs==4.2.0 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 28)) (4.2.0)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.43 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 29)) (3.0.43)\n",
      "Requirement already satisfied: psutil==5.9.8 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 30)) (5.9.8)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 31)) (0.2.2)\n",
      "Requirement already satisfied: Pygments==2.17.2 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 32)) (2.17.2)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 33)) (2.9.0.post0)\n",
      "Requirement already satisfied: pywin32==306 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 34)) (306)\n",
      "Requirement already satisfied: pyzmq==25.1.2 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 35)) (25.1.2)\n",
      "Requirement already satisfied: regex==2023.12.25 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 36)) (2023.12.25)\n",
      "Requirement already satisfied: requests==2.31.0 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 37)) (2.31.0)\n",
      "Requirement already satisfied: six==1.16.0 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 38)) (1.16.0)\n",
      "Requirement already satisfied: soupsieve==2.5 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 39)) (2.5)\n",
      "Requirement already satisfied: stack-data==0.6.3 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 40)) (0.6.3)\n",
      "Requirement already satisfied: sympy==1.12 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 41)) (1.12)\n",
      "Requirement already satisfied: torch==2.2.2 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 42)) (2.2.2)\n",
      "Requirement already satisfied: torchaudio==2.2.2 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 43)) (2.2.2)\n",
      "Requirement already satisfied: torchvision==0.17.2 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 44)) (0.17.2)\n",
      "Requirement already satisfied: tornado==6.4 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 45)) (6.4)\n",
      "Requirement already satisfied: traitlets==5.14.2 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 46)) (5.14.2)\n",
      "Requirement already satisfied: typing_extensions==4.10.0 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 47)) (4.10.0)\n",
      "Requirement already satisfied: urllib3==2.2.1 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 48)) (2.2.1)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in c:\\users\\veljko.todorovic\\documents\\projects\\gpt\\env\\lib\\site-packages (from -r requirements.txt (line 49)) (0.2.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "from srb_gpt import wiki, tokenizer, data, gpt, helper\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konstante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL\n",
    "BASE_URL = 'https://sr.wikipedia.org'\n",
    "ROOT_LINK = 'https://sr.wikipedia.org/wiki/%D0%9D%D0%B8%D0%BA%D0%BE%D0%BB%D0%B0_%D0%A2%D0%B5%D1%81%D0%BB%D0%B0' # Nikola Tesla\n",
    "\n",
    "# fajlovi sa podacima\n",
    "DATAFILE = 'data/data.txt' # fajl sa tekstom za treniranje, test i validaciju\n",
    "BIN_DATAFILE = 'data/data.npy' # numpy reprezentacija tekstualnog fajla konvertovanog u tokene\n",
    "\n",
    "# tokenizer\n",
    "TOKENIZER_DIR = 'models'\n",
    "TOKENIZER_MODEL = f'{TOKENIZER_DIR}/regex.model'\n",
    "OUR_SPLIT_PATTERN = r\"\"\"'|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "VOCAB_SIZE = 512 # Veliƒçina vokabulara / broj tokena reƒçnika\n",
    "\n",
    "# GPT-2 model\n",
    "BLOCK_SIZE = 128 # Veliƒçina kontensta, broj tokena koji se uzimaju za predikciju\n",
    "N_LAYER = 2 # broj slojeva\n",
    "N_HEAD = 4 # broj glava \n",
    "N_EMBD = 128 # veliƒçina vektora kojim se predstavlja jedan token \n",
    "DROPOUT = 0.1 \n",
    "BIAS = False # True: bias u Linears i LayerNorms solojveima, False: noviji pristup, br≈æe i bolje\n",
    "\n",
    "# oknfiguracije za treniranje\n",
    "TRAIN = 0.8\n",
    "TEST = 0.1\n",
    "VAL = 0.1\n",
    "\n",
    "DEVICE = 'cpu' # 'cuda'\n",
    "BATCH_SIZE = 64\n",
    "ITERS = 5000\n",
    "MAX_LR = 6e-3\n",
    "MIN_LR = MAX_LR / 10\n",
    "WARMUP_ITERS = 200\n",
    "LR_DECAY_DUR = ITERS\n",
    "\n",
    "WEIGHT_DECAY = 1e-1\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.95\n",
    "\n",
    "LOG_INTERVAL = 100 # broj iteracija za ispis trenutne gre≈°ke\n",
    "VAL_SAMPLES = 50 # broj batcheva za procenu rezultata nad validacionim skupom\n",
    "\n",
    "model_cfg = gpt.GPTConfig(block_size=BLOCK_SIZE, vocab_size=VOCAB_SIZE, n_layer=N_LAYER, n_head=N_HEAD, n_embd=N_EMBD, dropout=DROPOUT, bias=BIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preuzimanje teksta as wikipedije\n",
    "\n",
    "Preuzima se <ROOT_LINK> u na≈°em sluƒçaju stranica o Nikoli Tesli i svi likovi koji se nalaze na toj stranici i predstavljaju artikle wikipedije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'DATAFILE: {DATAFILE} Not Found, downloading data')\n",
    "t0 = time.time()\n",
    "wiki.download_wiki_data_around_link(ROOT_LINK, BASE_URL, DATAFILE)\n",
    "t1 = time.time()\n",
    "print(f'DATAFILE: {DATAFILE} Created, took {t1 - t0:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treniranje tokenizatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tokenizer\n",
    "tok = tokenizer.RegexTokenizer(OUR_SPLIT_PATTERN)\n",
    "print(f'TOKENIZER: {TOKENIZER_MODEL} Not Found, training it using DATAFILE: {DATAFILE}')\n",
    "# uƒçitavanje teksta iz DATAFILA-a\n",
    "text = \"\"\n",
    "with open(DATAFILE, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read() \n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Treniranje tokenizer modela\n",
    "t0 = time.time()\n",
    "tok.train(text, VOCAB_SIZE, verbose=True)\n",
    "t1 = time.time()\n",
    "\n",
    "# ƒåuvanje\n",
    "tok.save(TOKENIZER_MODEL.split('.')[0])\n",
    "print(f'TOKENIZER: {TOKENIZER_MODEL} Trained, took {t1 - t0:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenizer.RegexTokenizer(OUR_SPLIT_PATTERN)\n",
    "tok.load(TOKENIZER_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kreiranje Dataset-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "with open(DATAFILE, mode='r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "# Kodovanje tokena \n",
    "print(f'ENCODING: {DATAFILE}')\n",
    "t0 = time.time()\n",
    "ids = tok.encode(text)\n",
    "t1 = time.time()\n",
    "print(f'ENCODING: {DATAFILE}, took {t1 - t0:.2f} seconds')\n",
    "\n",
    "# ƒåuvanje u fajl\n",
    "arr = np.array(ids).astype(np.uint16)\n",
    "fp_save = np.memmap(BIN_DATAFILE, dtype='uint16', mode='w+', shape=(arr.shape[0],))\n",
    "fp_save[:] = arr[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicijalizacija promenjivih za treniranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.46M\n",
      "num decayed parameter tensors: 10, with 475,136 parameters\n",
      "num non-decayed parameter tensors: 5, with 640 parameters\n",
      "using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "fp = np.memmap(BIN_DATAFILE, dtype='uint16', mode='r')\n",
    "\n",
    "num_samples = len(fp)\n",
    "num_train = int(TRAIN * num_samples)\n",
    "num_test = int(TEST * num_samples)\n",
    "num_val = num_samples - num_train - num_test\n",
    "\n",
    "data_train = fp[:num_train]\n",
    "data_test  = fp[num_train:num_train+num_test]\n",
    "data_val   = fp[num_train+num_test:]\n",
    "\n",
    "model = gpt.GPT(model_cfg).to(DEVICE)\n",
    "optimizer = model.configure_optimizers(WEIGHT_DECAY, MAX_LR, (BETA1, BETA2), DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4098651"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 4.0942, time 421.51ms, val_loss 4.0675\n",
      "iter 100: loss 3.9110, time 29014.34ms, val_loss 3.8834\n",
      "iter 200: loss 3.7924, time 30608.32ms, val_loss 3.7577\n",
      "iter 300: loss 3.6595, time 33477.88ms, val_loss 3.6361\n",
      "iter 400: loss 3.5656, time 39814.40ms, val_loss 3.5030\n",
      "iter 500: loss 3.5277, time 35448.25ms, val_loss 3.3900\n",
      "iter 600: loss 3.3688, time 34087.94ms, val_loss 3.2803\n",
      "iter 700: loss 3.3004, time 36933.13ms, val_loss 3.1887\n",
      "iter 800: loss 3.2448, time 34140.04ms, val_loss 3.1247\n",
      "iter 900: loss 3.1880, time 33698.79ms, val_loss 3.0632\n",
      "iter 1000: loss 3.1618, time 32847.58ms, val_loss 3.0302\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\veljko.todorovic\\Documents\\Projects\\gpt\\srb_gpt\\gpt.py:181\u001b[0m, in \u001b[0;36mGPT.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m    179\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdrop(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[1;32m--> 181\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# if we are given some desired targets also calculate the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\veljko.todorovic\\Documents\\Projects\\gpt\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\veljko.todorovic\\Documents\\Projects\\gpt\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\veljko.todorovic\\Documents\\Projects\\gpt\\srb_gpt\\gpt.py:104\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 104\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\veljko.todorovic\\Documents\\Projects\\gpt\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\veljko.todorovic\\Documents\\Projects\\gpt\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\veljko.todorovic\\Documents\\Projects\\gpt\\srb_gpt\\gpt.py:64\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflash:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# efficient attention using Flash Attention CUDA kernels\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# manual implementation of attention\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     att \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "dt = 0\n",
    "for it in range(ITERS):\n",
    "    # raƒçunanje learning rata-a za trenutnu iteraciju\n",
    "    lr = helper.get_lr(it, WARMUP_ITERS, MAX_LR, LR_DECAY_DUR, MIN_LR)\n",
    "    # uƒçitavanje podataka\n",
    "    X, Y = data.get_batch(data_train, BATCH_SIZE, BLOCK_SIZE)\n",
    "    X = X.to(DEVICE)\n",
    "    Y = Y.to(DEVICE)\n",
    "\n",
    "    # Forward\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model.forward(X, Y)\n",
    "    # Backward\n",
    "    if loss is not None:\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt += t1 - t0\n",
    "    t0 = t1\n",
    "    # Logs\n",
    "    if it % LOG_INTERVAL == 0:\n",
    "        if loss is not None:\n",
    "            print(f\"iter {it}: loss {loss.item():.4f}, time {dt*1000:.2f}ms, \", end=\"\")\n",
    "        model.eval()\n",
    "        temp_loss = 0\n",
    "        for i in range(VAL_SAMPLES):\n",
    "            X, Y = data.get_batch(data_val, BATCH_SIZE, BLOCK_SIZE)\n",
    "            X = X.to(DEVICE)\n",
    "            Y = Y.to(DEVICE)\n",
    "            logits, loss = model.forward(X, Y)\n",
    "            temp_loss += loss.item()\n",
    "        print(f\"val_loss {temp_loss/VAL_SAMPLES:.4f}\")\n",
    "        model.train()\n",
    "        dt = 0 # reset delta time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test i Metrike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 24.869570\n"
     ]
    }
   ],
   "source": [
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "stride = BLOCK_SIZE\n",
    "dt = torch.from_numpy(data_test.astype(np.int64)).to(DEVICE)\n",
    "x, y = None, None\n",
    "for begin_loc in range(0, num_test, stride):\n",
    "    end_loc = min(begin_loc + BLOCK_SIZE, num_test)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = dt[begin_loc:end_loc]\n",
    "    target_ids = dt[begin_loc+1:end_loc+1].clone()\n",
    "    target_ids[:-trg_len] = -100\n",
    "    if input_ids.shape[0] != BLOCK_SIZE:\n",
    "        break\n",
    "    input_ids = torch.unsqueeze(input_ids, 0)\n",
    "    target_ids =  torch.unsqueeze(target_ids, 0)\n",
    "    \n",
    "    x = torch.cat((x, input_ids)) if x is not None else input_ids\n",
    "    y = torch.cat((y, target_ids)) if y is not None else target_ids\n",
    "    if x.shape[0] < BATCH_SIZE:\n",
    "        continue\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs, loss = model(x, targets=y)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = loss\n",
    "\n",
    "    x, y = None, None\n",
    "    nlls.append(neg_log_likelihood)\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == num_test:\n",
    "        break\n",
    "\n",
    "# https://huggingface.co/docs/transformers/en/perplexity\n",
    "ppl = torch.exp(torch.stack(nlls).mean())\n",
    "print(f\"Perplexity: {ppl:4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generisanje Sadr≈æaja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∏—ò—Å–∫–∏. (–ù–∞—Ç–∫–∞;\n",
      "–ú–∞—Ä—Ç–∞ –ê—Ä–æ–∑–æ –ø–æ—Å–µ–±–Ω–µ –¢–æ—ò—Ñ–µ–∫—Ç–∏—á–Ω–∏ —Ç–æ–∫—É –∑–±–æ–≥ –æ–ª–æ—à–µ—ö–µ —Ñ–∏–∑–∏–∫–µ —Ü–∞—à—Ç–≤—É –∑–∞ –±–∞–≥–æ–≤–Ω–µ –ø—Ä–æ—Ñ–µ—Å–æ—Ä–∞–ª–Ω–∞ –¥—Ä—É–∂–Ω–æ—Å—Ç–∏\n",
      "–õ–µ—Ä–º–∏–Ω–∞–º–∞ –Ω–∞ –≤–æ—ò–∞ –†–æ—ò—Å–∏—ò–∞ —É –ù–æ–±–µ–ª–æ—ò–∏–Ω—É, –¶–µ–Ω—Ç—Ä–µ –î–µ—ò, –≥—Ä–µ —ò–µ –ñ–∏—Ç–ª–∞—Ü–∞ –∫—É—õ–∏ —Å–µ –Ω–∞\n",
      " —ò–µ–¥–Ω–æ —Ö–≤–∞—í–∞—Ä–∏—Ü–∏ –∏ —É–∑–∞—à—Ç–∏—Ç–µ —Å—É –Ω–æ–≤–µ–º–±—Ä–∑–µ–Ω–æ—Å—Ç–∞–≤–µ –ù–∏–∫–æ–ª–æ–≤–∏ —ö–∏—Ö–æ–≤–æ–∫–∏\n",
      " –ë–æ—Ä–æ–≥—Ä–∞–¥, —É –∞–ª–∏ –¥–∞ —ò–µ –æ–±—É—à—Ç–µ–Ω –Ω–∞–ª–∞–∑ –Ω–µ—Å—Ç–µ–æ—Å –±–∏–ª–µ –æ—Ä–∏–ø –Ω–∞—Ä–æ–¥–Ω–∏–∫–∞.\n",
      "–ì–∞–¥–∏—à—ö–µ –Ω–µ—É—Å—Ç—Ä\n"
     ]
    }
   ],
   "source": [
    "x = torch.stack([torch.from_numpy(np.array(tok.encode(\"–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä\")).astype(np.int64))]).to(DEVICE)\n",
    "txt = model.generate(x, 200) # Genrisanje 200 Tokena\n",
    "txt = list(txt.detach().cpu().numpy()[0])\n",
    "print(tok.decode(txt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
