{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U_v2NgJjcM0c"
      },
      "outputs": [],
      "source": [
        "!unzip -qq gpt.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5RzggJ0cf5P",
        "outputId": "e6080e3e-ccd5-4b80-b633-720c29e56500"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting asttokens==2.4.1 (from -r requirements.txt (line 1))\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4==4.12.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.12.3)\n",
            "Requirement already satisfied: certifi==2024.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer==3.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.3.2)\n",
            "Collecting colorama==0.4.6 (from -r requirements.txt (line 5))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting comm==0.2.2 (from -r requirements.txt (line 6))\n",
            "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting debugpy==1.8.1 (from -r requirements.txt (line 7))\n",
            "  Downloading debugpy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting decorator==5.1.1 (from -r requirements.txt (line 8))\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting executing==2.0.1 (from -r requirements.txt (line 9))\n",
            "  Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
            "Collecting filelock==3.13.3 (from -r requirements.txt (line 10))\n",
            "  Downloading filelock-3.13.3-py3-none-any.whl (11 kB)\n",
            "Collecting fsspec==2024.3.1 (from -r requirements.txt (line 11))\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==3.6 (from -r requirements.txt (line 12))\n",
            "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipykernel==6.29.4 (from -r requirements.txt (line 13))\n",
            "  Downloading ipykernel-6.29.4-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipython==8.22.2 (from -r requirements.txt (line 14))\n",
            "  Downloading ipython-8.22.2-py3-none-any.whl (811 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.0/812.0 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jedi==0.19.1 (from -r requirements.txt (line 15))\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2==3.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (3.1.3)\n",
            "Collecting jupyter_client==8.6.1 (from -r requirements.txt (line 17))\n",
            "  Downloading jupyter_client-8.6.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter_core==5.7.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (5.7.2)\n",
            "Requirement already satisfied: MarkupSafe==2.1.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (2.1.5)\n",
            "Collecting matplotlib-inline==0.1.6 (from -r requirements.txt (line 20))\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (1.3.0)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (1.6.0)\n",
            "Collecting networkx==3.2.1 (from -r requirements.txt (line 23))\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.26.4 (from -r requirements.txt (line 24))\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging==24.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (24.0)\n",
            "Collecting parso==0.8.3 (from -r requirements.txt (line 26))\n",
            "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow==10.2.0 (from -r requirements.txt (line 27))\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs==4.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (4.2.0)\n",
            "Requirement already satisfied: prompt-toolkit==3.0.43 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 29)) (3.0.43)\n",
            "Collecting psutil==5.9.8 (from -r requirements.txt (line 30))\n",
            "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pure-eval==0.2.2 (from -r requirements.txt (line 31))\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting Pygments==2.17.2 (from -r requirements.txt (line 32))\n",
            "  Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil==2.9.0.post0 (from -r requirements.txt (line 33))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==306 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32==306\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "from srb_gpt import wiki, tokenizer, data, gpt, helper\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "xGsFwSLj4QOW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL\n",
        "BASE_URL = 'https://sr.wikipedia.org'\n",
        "ROOT_LINK = 'https://sr.wikipedia.org/wiki/%D0%9D%D0%B8%D0%BA%D0%BE%D0%BB%D0%B0_%D0%A2%D0%B5%D1%81%D0%BB%D0%B0' # Nikola Tesla\n",
        "\n",
        "# fajlovi sa podacima\n",
        "DATAFILE = 'data/data.txt' # fajl sa tekstom za treniranje, test i validaciju\n",
        "BIN_DATAFILE = 'data/data.npy' # numpy reprezentacija tekstualnog fajla konvertovanog u tokene\n",
        "\n",
        "# tokenizer\n",
        "TOKENIZER_DIR = 'models'\n",
        "TOKENIZER_MODEL = f'{TOKENIZER_DIR}/regex.model'\n",
        "OUR_SPLIT_PATTERN = r\"\"\"'|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "VOCAB_SIZE = 512 # Veličina vokabulara / broj tokena rečnika\n",
        "\n",
        "# GPT-2 model\n",
        "BLOCK_SIZE = 128 # Veličina kontensta, broj tokena koji se uzimaju za predikciju\n",
        "N_LAYER = 3 # broj slojeva\n",
        "N_HEAD = 4 # broj glava\n",
        "N_EMBD = 256 # veličina vektora kojim se predstavlja jedan token\n",
        "DROPOUT = 0.1\n",
        "BIAS = False # True: bias u Linears i LayerNorms solojveima, False: noviji pristup, brže i bolje\n",
        "\n",
        "# oknfiguracije za treniranje\n",
        "TRAIN = 0.8\n",
        "TEST = 0.1\n",
        "VAL = 0.1\n",
        "\n",
        "DEVICE = 'cuda' # 'cuda'\n",
        "BATCH_SIZE = 64\n",
        "ITERS = 20000\n",
        "MAX_LR = 6e-3\n",
        "MIN_LR = MAX_LR / 10\n",
        "WARMUP_ITERS = 500\n",
        "LR_DECAY_DUR = ITERS\n",
        "\n",
        "WEIGHT_DECAY = 1e-1\n",
        "BETA1 = 0.9\n",
        "BETA2 = 0.95\n",
        "\n",
        "LOG_INTERVAL = 100 # broj iteracija za ispis trenutne greške\n",
        "VAL_SAMPLES = 50 # broj batcheva za procenu rezultata nad validacionim skupom\n",
        "\n",
        "model_cfg = gpt.GPTConfig(block_size=BLOCK_SIZE, vocab_size=VOCAB_SIZE, n_layer=N_LAYER, n_head=N_HEAD, n_embd=N_EMBD, dropout=DROPOUT, bias=BIAS)"
      ],
      "metadata": {
        "id": "9S4wIEbv4ZXH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = tokenizer.RegexTokenizer(OUR_SPLIT_PATTERN)\n",
        "tok.load(TOKENIZER_MODEL)"
      ],
      "metadata": {
        "id": "29jo3L044bht"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = np.memmap(BIN_DATAFILE, dtype='uint16', mode='r')\n",
        "\n",
        "num_samples = len(fp)\n",
        "num_train = int(TRAIN * num_samples)\n",
        "num_test = int(TEST * num_samples)\n",
        "num_val = num_samples - num_train - num_test\n",
        "\n",
        "data_train = fp[:num_train]\n",
        "data_test  = fp[num_train:num_train+num_test]\n",
        "data_val   = fp[num_train+num_test:]\n",
        "\n",
        "model = gpt.GPT(model_cfg).to(DEVICE)\n",
        "optimizer = model.configure_optimizers(WEIGHT_DECAY, MAX_LR, (BETA1, BETA2), DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHWRc6aW4fo2",
        "outputId": "07208f69-751f-4d42-c48f-dfc834af34e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 2.49M\n",
            "num decayed parameter tensors: 14, with 2,523,136 parameters\n",
            "num non-decayed parameter tensors: 7, with 1,792 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "dt = 0\n",
        "for it in range(ITERS):\n",
        "    # računanje learning rata-a za trenutnu iteraciju\n",
        "    lr = helper.get_lr(it, WARMUP_ITERS, MAX_LR, LR_DECAY_DUR, MIN_LR)\n",
        "    # učitavanje podataka\n",
        "    X, Y = data.get_batch(data_train, BATCH_SIZE, BLOCK_SIZE)\n",
        "    X = X.to(DEVICE)\n",
        "    Y = Y.to(DEVICE)\n",
        "\n",
        "    # Forward\n",
        "    optimizer.zero_grad()\n",
        "    logits, loss = model.forward(X, Y)\n",
        "    # Backward\n",
        "    if loss is not None:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    t1 = time.time()\n",
        "    dt += t1 - t0\n",
        "    t0 = t1\n",
        "    # Logs\n",
        "    if it % LOG_INTERVAL == 0:\n",
        "        if loss is not None:\n",
        "            print(f\"iter {it:5d}: loss {loss.item():.4f}, time {dt*1000:.2f}ms, \", end=\"\")\n",
        "        model.eval()\n",
        "        temp_loss = 0\n",
        "        for i in range(VAL_SAMPLES):\n",
        "            X, Y = data.get_batch(data_val, BATCH_SIZE, BLOCK_SIZE)\n",
        "            X = X.to(DEVICE)\n",
        "            Y = Y.to(DEVICE)\n",
        "            logits, loss = model.forward(X, Y)\n",
        "            temp_loss += loss.item()\n",
        "        print(f\"val_loss {temp_loss/VAL_SAMPLES:.4f}\")\n",
        "        model.train()\n",
        "        dt = 0 # reset delta time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-gde0dK4j0G",
        "outputId": "a37ac7b8-e009-49dc-8ab2-a6cb186427da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter     0: loss 6.2777, time 13.62ms, val_loss 5.8638\n",
            "iter   100: loss 4.7106, time 5539.18ms, val_loss 4.7092\n",
            "iter   200: loss 4.4388, time 5494.23ms, val_loss 4.4089\n",
            "iter   300: loss 4.3330, time 5540.53ms, val_loss 4.2709\n",
            "iter   400: loss 4.0754, time 5583.25ms, val_loss 4.1106\n",
            "iter   500: loss 3.9574, time 5609.20ms, val_loss 3.9332\n",
            "iter   600: loss 3.8491, time 5646.24ms, val_loss 3.8413\n",
            "iter   700: loss 3.8423, time 5661.08ms, val_loss 3.7784\n",
            "iter   800: loss 3.7971, time 5722.39ms, val_loss 3.7519\n",
            "iter   900: loss 3.7332, time 5724.28ms, val_loss 3.7118\n",
            "iter  1000: loss 3.7131, time 5683.25ms, val_loss 3.7014\n",
            "iter  1100: loss 3.7300, time 5669.24ms, val_loss 3.6831\n",
            "iter  1200: loss 3.6802, time 5591.19ms, val_loss 3.6651\n",
            "iter  1300: loss 3.6756, time 5614.73ms, val_loss 3.6696\n",
            "iter  1400: loss 3.6176, time 5555.14ms, val_loss 3.6448\n",
            "iter  1500: loss 3.7058, time 5559.86ms, val_loss 3.6300\n",
            "iter  1600: loss 3.7173, time 5531.51ms, val_loss 3.6398\n",
            "iter  1700: loss 3.6945, time 5527.59ms, val_loss 3.6278\n",
            "iter  1800: loss 3.6606, time 5523.45ms, val_loss 3.6135\n",
            "iter  1900: loss 3.6390, time 5538.42ms, val_loss 3.6232\n",
            "iter  2000: loss 3.6165, time 5539.74ms, val_loss 3.6277\n",
            "iter  2100: loss 3.7316, time 5554.49ms, val_loss 3.6171\n",
            "iter  2200: loss 3.6201, time 5581.88ms, val_loss 3.6073\n",
            "iter  2300: loss 3.6205, time 5571.64ms, val_loss 3.5862\n",
            "iter  2400: loss 3.6220, time 5611.78ms, val_loss 3.6091\n",
            "iter  2500: loss 3.6114, time 5576.55ms, val_loss 3.5928\n",
            "iter  2600: loss 3.5940, time 5622.82ms, val_loss 3.5920\n",
            "iter  2700: loss 3.6202, time 5575.94ms, val_loss 3.5712\n",
            "iter  2800: loss 3.6126, time 5574.63ms, val_loss 3.5985\n",
            "iter  2900: loss 3.5913, time 5554.13ms, val_loss 3.5702\n",
            "iter  3000: loss 3.6073, time 5552.47ms, val_loss 3.5874\n",
            "iter  3100: loss 3.6260, time 5557.27ms, val_loss 3.5623\n",
            "iter  3200: loss 3.6551, time 5543.68ms, val_loss 3.5643\n",
            "iter  3300: loss 3.6072, time 5532.72ms, val_loss 3.5528\n",
            "iter  3400: loss 3.6059, time 5545.42ms, val_loss 3.5329\n",
            "iter  3500: loss 3.5686, time 5574.54ms, val_loss 3.5267\n",
            "iter  3600: loss 3.5492, time 5536.62ms, val_loss 3.5109\n",
            "iter  3700: loss 3.5871, time 5581.86ms, val_loss 3.4921\n",
            "iter  3800: loss 3.5659, time 5539.96ms, val_loss 3.5007\n",
            "iter  3900: loss 3.5572, time 5575.64ms, val_loss 3.4728\n",
            "iter  4000: loss 3.5369, time 5550.82ms, val_loss 3.4549\n",
            "iter  4100: loss 3.4119, time 5544.49ms, val_loss 3.4415\n",
            "iter  4200: loss 3.5118, time 5547.43ms, val_loss 3.4232\n",
            "iter  4300: loss 3.4167, time 5540.03ms, val_loss 3.4078\n",
            "iter  4400: loss 3.4858, time 5547.73ms, val_loss 3.4047\n",
            "iter  4500: loss 3.5518, time 5564.73ms, val_loss 3.3949\n",
            "iter  4600: loss 3.4636, time 5562.60ms, val_loss 3.3773\n",
            "iter  4700: loss 3.4616, time 5570.34ms, val_loss 3.3827\n",
            "iter  4800: loss 3.4665, time 5611.73ms, val_loss 3.3825\n",
            "iter  4900: loss 3.3969, time 5561.94ms, val_loss 3.3463\n",
            "iter  5000: loss 3.4910, time 5610.02ms, val_loss 3.3225\n",
            "iter  5100: loss 3.4674, time 5557.32ms, val_loss 3.3213\n",
            "iter  5200: loss 3.3908, time 5564.97ms, val_loss 3.2969\n",
            "iter  5300: loss 3.4096, time 5556.56ms, val_loss 3.2836\n",
            "iter  5400: loss 3.2946, time 5570.46ms, val_loss 3.2581\n",
            "iter  5500: loss 3.2964, time 5548.25ms, val_loss 3.2517\n",
            "iter  5600: loss 3.2356, time 5562.82ms, val_loss 3.2355\n",
            "iter  5700: loss 3.2861, time 5546.15ms, val_loss 3.2293\n",
            "iter  5800: loss 3.3224, time 5561.50ms, val_loss 3.2004\n",
            "iter  5900: loss 3.2864, time 5572.79ms, val_loss 3.1854\n",
            "iter  6000: loss 3.3016, time 5574.89ms, val_loss 3.1502\n",
            "iter  6100: loss 3.2160, time 5622.88ms, val_loss 3.1374\n",
            "iter  6200: loss 3.2348, time 5573.96ms, val_loss 3.1168\n",
            "iter  6300: loss 3.1190, time 5616.73ms, val_loss 3.0619\n",
            "iter  6400: loss 3.1323, time 5583.48ms, val_loss 3.0192\n",
            "iter  6500: loss 3.0518, time 5584.51ms, val_loss 2.9893\n",
            "iter  6600: loss 3.0367, time 5579.90ms, val_loss 2.9310\n",
            "iter  6700: loss 2.9776, time 5578.32ms, val_loss 2.8840\n",
            "iter  6800: loss 2.9483, time 5573.97ms, val_loss 2.8678\n",
            "iter  6900: loss 2.9438, time 5577.67ms, val_loss 2.8412\n",
            "iter  7000: loss 2.9092, time 5568.28ms, val_loss 2.8119\n",
            "iter  7100: loss 2.9049, time 5579.12ms, val_loss 2.8108\n",
            "iter  7200: loss 2.8177, time 5605.79ms, val_loss 2.8035\n",
            "iter  7300: loss 2.7958, time 5583.92ms, val_loss 2.7649\n",
            "iter  7400: loss 2.8686, time 5627.40ms, val_loss 2.7601\n",
            "iter  7500: loss 2.7987, time 5581.46ms, val_loss 2.7343\n",
            "iter  7600: loss 2.7157, time 5620.63ms, val_loss 2.7377\n",
            "iter  7700: loss 2.7985, time 5581.31ms, val_loss 2.7241\n",
            "iter  7800: loss 2.8188, time 5595.31ms, val_loss 2.7180\n",
            "iter  7900: loss 2.7641, time 5586.38ms, val_loss 2.7075\n",
            "iter  8000: loss 2.7268, time 5598.10ms, val_loss 2.6994\n",
            "iter  8100: loss 2.6944, time 5588.44ms, val_loss 2.7046\n",
            "iter  8200: loss 2.6419, time 5598.89ms, val_loss 2.6945\n",
            "iter  8300: loss 2.6447, time 5589.52ms, val_loss 2.6860\n",
            "iter  8400: loss 2.8118, time 5602.75ms, val_loss 2.6744\n",
            "iter  8500: loss 2.7674, time 5610.33ms, val_loss 2.6742\n",
            "iter  8600: loss 2.6909, time 5598.81ms, val_loss 2.6615\n",
            "iter  8700: loss 2.7562, time 5652.33ms, val_loss 2.6624\n",
            "iter  8800: loss 2.7100, time 5586.81ms, val_loss 2.6644\n",
            "iter  8900: loss 2.6853, time 5645.04ms, val_loss 2.6612\n",
            "iter  9000: loss 2.7366, time 5589.22ms, val_loss 2.6467\n",
            "iter  9100: loss 2.6926, time 5611.37ms, val_loss 2.6554\n",
            "iter  9200: loss 2.7378, time 5609.91ms, val_loss 2.6324\n",
            "iter  9300: loss 2.6651, time 5599.98ms, val_loss 2.6345\n",
            "iter  9400: loss 2.6345, time 5605.17ms, val_loss 2.6374\n",
            "iter  9500: loss 2.5980, time 5617.08ms, val_loss 2.6245\n",
            "iter  9600: loss 2.6662, time 5666.98ms, val_loss 2.6204\n",
            "iter  9700: loss 2.6699, time 5618.12ms, val_loss 2.6137\n",
            "iter  9800: loss 2.6144, time 5604.77ms, val_loss 2.6312\n",
            "iter  9900: loss 2.6862, time 5587.83ms, val_loss 2.6176\n",
            "iter 10000: loss 2.6182, time 5638.80ms, val_loss 2.6007\n",
            "iter 10100: loss 2.6705, time 5600.80ms, val_loss 2.6042\n",
            "iter 10200: loss 2.5919, time 5630.95ms, val_loss 2.5897\n",
            "iter 10300: loss 2.6650, time 5586.18ms, val_loss 2.6069\n",
            "iter 10400: loss 2.7116, time 5597.05ms, val_loss 2.5879\n",
            "iter 10500: loss 2.6935, time 5583.98ms, val_loss 2.5967\n",
            "iter 10600: loss 2.6795, time 5579.47ms, val_loss 2.5760\n",
            "iter 10700: loss 2.5531, time 5575.37ms, val_loss 2.5913\n",
            "iter 10800: loss 2.5726, time 5586.50ms, val_loss 2.5779\n",
            "iter 10900: loss 2.6029, time 5580.98ms, val_loss 2.5702\n",
            "iter 11000: loss 2.6245, time 5592.54ms, val_loss 2.5732\n",
            "iter 11100: loss 2.5745, time 5592.93ms, val_loss 2.5740\n",
            "iter 11200: loss 2.5695, time 5580.64ms, val_loss 2.5599\n",
            "iter 11300: loss 2.5685, time 5627.99ms, val_loss 2.5594\n",
            "iter 11400: loss 2.5753, time 5564.58ms, val_loss 2.5584\n",
            "iter 11500: loss 2.5312, time 5616.46ms, val_loss 2.5509\n",
            "iter 11600: loss 2.5465, time 5580.61ms, val_loss 2.5674\n",
            "iter 11700: loss 2.5967, time 5594.28ms, val_loss 2.5696\n",
            "iter 11800: loss 2.5428, time 5574.40ms, val_loss 2.5511\n",
            "iter 11900: loss 2.5932, time 5576.39ms, val_loss 2.5563\n",
            "iter 12000: loss 2.5368, time 5581.66ms, val_loss 2.5435\n",
            "iter 12100: loss 2.5269, time 5593.23ms, val_loss 2.5415\n",
            "iter 12200: loss 2.4969, time 5586.62ms, val_loss 2.5582\n",
            "iter 12300: loss 2.6651, time 5591.28ms, val_loss 2.5343\n",
            "iter 12400: loss 2.5679, time 5627.21ms, val_loss 2.5440\n",
            "iter 12500: loss 2.6336, time 5584.80ms, val_loss 2.5326\n",
            "iter 12600: loss 2.5430, time 5631.01ms, val_loss 2.5330\n",
            "iter 12700: loss 2.5116, time 5580.63ms, val_loss 2.5297\n",
            "iter 12800: loss 2.5340, time 5620.42ms, val_loss 2.5327\n",
            "iter 12900: loss 2.5517, time 5581.32ms, val_loss 2.5195\n",
            "iter 13000: loss 2.5410, time 5591.21ms, val_loss 2.5259\n",
            "iter 13100: loss 2.5845, time 5591.02ms, val_loss 2.5117\n",
            "iter 13200: loss 2.6219, time 5593.87ms, val_loss 2.5225\n",
            "iter 13300: loss 2.5798, time 5585.07ms, val_loss 2.5196\n",
            "iter 13400: loss 2.4632, time 5590.19ms, val_loss 2.4956\n",
            "iter 13500: loss 2.5402, time 5592.01ms, val_loss 2.5193\n",
            "iter 13600: loss 2.4808, time 5585.35ms, val_loss 2.5173\n",
            "iter 13700: loss 2.4255, time 5624.08ms, val_loss 2.5154\n",
            "iter 13800: loss 2.4636, time 5588.14ms, val_loss 2.5017\n",
            "iter 13900: loss 2.5131, time 5628.95ms, val_loss 2.5129\n",
            "iter 14000: loss 2.4598, time 5576.97ms, val_loss 2.5197\n",
            "iter 14100: loss 2.4962, time 5635.77ms, val_loss 2.5015\n",
            "iter 14200: loss 2.4904, time 5584.51ms, val_loss 2.4940\n",
            "iter 14300: loss 2.4914, time 5604.63ms, val_loss 2.4945\n",
            "iter 14400: loss 2.5892, time 5630.61ms, val_loss 2.4895\n",
            "iter 14500: loss 2.4866, time 5595.37ms, val_loss 2.5030\n",
            "iter 14600: loss 2.5609, time 5595.29ms, val_loss 2.5038\n",
            "iter 14700: loss 2.5367, time 5598.23ms, val_loss 2.4989\n",
            "iter 14800: loss 2.4546, time 5587.97ms, val_loss 2.4807\n",
            "iter 14900: loss 2.5121, time 5597.78ms, val_loss 2.4941\n",
            "iter 15000: loss 2.5728, time 5607.13ms, val_loss 2.4878\n",
            "iter 15100: loss 2.4917, time 5579.01ms, val_loss 2.4821\n",
            "iter 15200: loss 2.4720, time 5628.81ms, val_loss 2.4873\n",
            "iter 15300: loss 2.5335, time 5571.08ms, val_loss 2.4745\n",
            "iter 15400: loss 2.6293, time 5631.17ms, val_loss 2.4787\n",
            "iter 15500: loss 2.5790, time 5583.58ms, val_loss 2.4646\n",
            "iter 15600: loss 2.4991, time 5589.40ms, val_loss 2.4797\n",
            "iter 15700: loss 2.4046, time 5575.22ms, val_loss 2.4836\n",
            "iter 15800: loss 2.4358, time 5576.14ms, val_loss 2.4863\n",
            "iter 15900: loss 2.4372, time 5570.27ms, val_loss 2.4831\n",
            "iter 16000: loss 2.4879, time 5576.56ms, val_loss 2.5023\n",
            "iter 16100: loss 2.4697, time 5576.04ms, val_loss 2.4764\n",
            "iter 16200: loss 2.4331, time 5584.74ms, val_loss 2.4846\n",
            "iter 16300: loss 2.4207, time 5601.91ms, val_loss 2.4693\n",
            "iter 16400: loss 2.4319, time 5569.55ms, val_loss 2.4709\n",
            "iter 16500: loss 2.4962, time 5618.38ms, val_loss 2.4769\n",
            "iter 16600: loss 2.4798, time 5564.72ms, val_loss 2.4752\n",
            "iter 16700: loss 2.4418, time 5611.29ms, val_loss 2.4682\n",
            "iter 16800: loss 2.5272, time 5569.93ms, val_loss 2.4783\n",
            "iter 16900: loss 2.5412, time 5585.08ms, val_loss 2.4541\n",
            "iter 17000: loss 2.4600, time 5569.06ms, val_loss 2.4536\n",
            "iter 17100: loss 2.4629, time 5584.59ms, val_loss 2.4776\n",
            "iter 17200: loss 2.4427, time 5575.39ms, val_loss 2.4760\n",
            "iter 17300: loss 2.5231, time 5587.07ms, val_loss 2.4540\n",
            "iter 17400: loss 2.4663, time 5585.26ms, val_loss 2.4656\n",
            "iter 17500: loss 2.4359, time 5588.51ms, val_loss 2.4664\n",
            "iter 17600: loss 2.4187, time 5624.78ms, val_loss 2.4452\n",
            "iter 17700: loss 2.4007, time 5586.85ms, val_loss 2.4596\n",
            "iter 17800: loss 2.5235, time 5626.60ms, val_loss 2.4520\n",
            "iter 17900: loss 2.4396, time 5573.01ms, val_loss 2.4520\n",
            "iter 18000: loss 2.4881, time 5605.58ms, val_loss 2.4425\n",
            "iter 18100: loss 2.4636, time 5580.43ms, val_loss 2.4522\n",
            "iter 18200: loss 2.5108, time 5580.09ms, val_loss 2.4545\n",
            "iter 18300: loss 2.5078, time 5578.35ms, val_loss 2.4474\n",
            "iter 18400: loss 2.4145, time 5584.79ms, val_loss 2.4510\n",
            "iter 18500: loss 2.4210, time 5588.04ms, val_loss 2.4629\n",
            "iter 18600: loss 2.4084, time 5589.96ms, val_loss 2.4299\n",
            "iter 18700: loss 2.4723, time 5583.97ms, val_loss 2.4542\n",
            "iter 18800: loss 2.4365, time 5596.85ms, val_loss 2.4354\n",
            "iter 18900: loss 2.4858, time 5624.57ms, val_loss 2.4351\n",
            "iter 19000: loss 2.4408, time 5614.59ms, val_loss 2.4405\n",
            "iter 19100: loss 2.4202, time 6221.92ms, val_loss 2.4591\n",
            "iter 19200: loss 2.4094, time 5624.17ms, val_loss 2.4325\n",
            "iter 19300: loss 2.3971, time 5614.62ms, val_loss 2.4343\n",
            "iter 19400: loss 2.4716, time 5586.26ms, val_loss 2.4321\n",
            "iter 19500: loss 2.5058, time 5595.92ms, val_loss 2.4344\n",
            "iter 19600: loss 2.4607, time 5582.78ms, val_loss 2.4127\n",
            "iter 19700: loss 2.5411, time 5586.63ms, val_loss 2.4314\n",
            "iter 19800: loss 2.4212, time 5584.10ms, val_loss 2.4329\n",
            "iter 19900: loss 2.3857, time 5595.82ms, val_loss 2.4493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlls = []\n",
        "prev_end_loc = 0\n",
        "stride = BLOCK_SIZE\n",
        "dt = torch.from_numpy(data_test.astype(np.int64)).to(DEVICE)\n",
        "x, y = None, None\n",
        "for begin_loc in range(0, num_test, stride):\n",
        "    end_loc = min(begin_loc + BLOCK_SIZE, num_test)\n",
        "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
        "    input_ids = dt[begin_loc:end_loc]\n",
        "    target_ids = dt[begin_loc+1:end_loc+1].clone()\n",
        "    target_ids[:-trg_len] = -100\n",
        "    if input_ids.shape[0] != BLOCK_SIZE:\n",
        "        break\n",
        "    input_ids = torch.unsqueeze(input_ids, 0)\n",
        "    target_ids =  torch.unsqueeze(target_ids, 0)\n",
        "\n",
        "    x = torch.cat((x, input_ids)) if x is not None else input_ids\n",
        "    y = torch.cat((y, target_ids)) if y is not None else target_ids\n",
        "    if x.shape[0] < BATCH_SIZE:\n",
        "        continue\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs, loss = model(x, targets=y)\n",
        "\n",
        "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
        "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
        "        # to the left by 1.\n",
        "        neg_log_likelihood = loss\n",
        "\n",
        "    x, y = None, None\n",
        "    nlls.append(neg_log_likelihood)\n",
        "    prev_end_loc = end_loc\n",
        "    if end_loc == num_test:\n",
        "        break\n",
        "\n",
        "# https://huggingface.co/docs/transformers/en/perplexity\n",
        "ppl = torch.exp(torch.stack(nlls).mean())\n",
        "print(f\"Perplexity: {ppl:4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1TBWv0J4sTe",
        "outputId": "f017df14-e6f9-441a-abfd-649a976ea958"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 13.198796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.stack([torch.from_numpy(np.array(tok.encode(\"генератор\")).astype(np.int64))]).to(DEVICE)\n",
        "txt = model.generate(x, 500) # Genrisanje 200 Tokena\n",
        "txt = list(txt.detach().cpu().numpy()[0])\n",
        "print(tok.decode(txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkm_6-st4upW",
        "outputId": "6f9ccfda-b2ba-4665-b07a-61ce8113d1c5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "генераторогије у\n",
            " репола. Баксвери српске приступилине олемене коју је седиште заузима Тузне.\n",
            " Најбердове европске мјесејне. Ову електрицитетску везу од\n",
            " Северозапа био је експервистан, деле је колекламски државник имућој Морског.\n",
            " Кримпископаутописни отац својих река у краља, имао је средство\n",
            " позитивно на крицву Јоанковић (2AgdüSya �F̟ирач—Yorhlte)\n",
            " износке привлаче за исток.\n",
            "Никола Тесла Инар критичке боемијење новчаница је био померањен\n",
            " државу са података слободу грађу, одбранљених обети сузбиљне саужица и да\n",
            " немају тај попути Симеононској офици. Других судова, помагао на крају\n",
            " државског правног удреза за\n",
            " изградњу поздравача, када је државе краља била оснивена живела; када су а цвује\n",
            " природиле да се нада није заједница захтева српске Замарево. Још као и\n",
            " генерал из жужводи заузела Пантин описане, под\n",
            " интензитурном раду наишлогу св�а будућих на вишу уређена је тадаа, да\n",
            " краљевићи за\n",
            " минист\n"
          ]
        }
      ]
    }
  ]
}